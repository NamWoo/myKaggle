{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "House_Price_Prediction_20190422_03_sub.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "YgqfV1Z2G_WS",
        "colab_type": "code",
        "outputId": "87f43791-d414-4f90-9d7f-b62b5c19c24a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l8wmAVtoHYbC",
        "colab_type": "code",
        "outputId": "6cd081ec-60de-4532-fb71-e254d49d53f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/c4/f130237b24efd1941cb685da12496675a90045129b66774751f1bf629dfd/catboost-0.14.2-cp36-none-manylinux1_x86_64.whl (60.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 60.6MB 757kB/s \n",
            "\u001b[?25hRequirement already satisfied: enum34 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.1.6)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.16.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.11.0)\n",
            "Requirement already satisfied: pandas>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from catboost) (0.23.4)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.1->catboost) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.1->catboost) (2.5.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-0.14.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z3v8WZasHSRs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import KFold \n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "from time import localtime\n",
        "import time\n",
        "from datetime import datetime, timedelta,date\n",
        "import gc\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.tree  import DecisionTreeRegressor\n",
        "\n",
        "from sklearn.metrics import explained_variance_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# from plotnine import *\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import ElasticNet, Lasso, Ridge\n",
        "import catboost as cb\n",
        "from functools import wraps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Oj3N9F7HdQq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.style.use('ggplot')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "pd.options.display.max_rows = 10000\n",
        "pd.options.display.max_columns = 10000\n",
        "pd.options.display.max_colwidth = 1000\n",
        "\n",
        "RANDOM_SEED = 631\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "path = \"/content/\"\n",
        "\n",
        "train = pd.read_csv(path+\"train.csv\", index_col=0)\n",
        "test = pd.read_csv(path+\"test.csv\", index_col=0)\n",
        "\n",
        "price_raw = train['price']\n",
        "price_raw_log = np.log1p(price_raw)\n",
        "\n",
        "train.drop('price', axis = 1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uiCqT_V6G0Q9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# type check\n",
        "# date column is a object type\n",
        "def type_checkk(dataset):\n",
        "    for i in dataset.columns:\n",
        "        colty = dataset[i].dtype\n",
        "        if not colty == 'int64' and not colty == 'float64':\n",
        "            print(i.ljust(15),'column is a', str(dataset[i].dtype).ljust(8), 'type')\n",
        "\n",
        "\n",
        "def ch_model(X_train, y_train=price_raw):  \n",
        "    xgb_params_add1 ={\n",
        "        'seed': RANDOM_SEED,\n",
        "        'learning_rate': 0.05,\n",
        "        'max_depth': 5,\n",
        "        'subsample': 0.9,\n",
        "        'colsample_bytree': 0.4,\n",
        "        'silent': True,\n",
        "        # 'gpu_id':0 ,         \n",
        "        # 'tree_method':'gpu_hist',\n",
        "        # 'predictor':'gpu_predictor',\n",
        "        'n_estimators':5000,\n",
        "        'refit' : True\n",
        "    }\n",
        "\n",
        "    ch_xgb_model = xgb.XGBRegressor() \n",
        "    #   x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.1, random_state=RANDOM_SEED)\n",
        "    dtrain = xgb.DMatrix(X_train, y_train)\n",
        "    # dtest = xgb.DMatrix(X_test)\n",
        "\n",
        "    # cross validation\n",
        "    cv_out = xgb.cv(xgb_params_add1,\n",
        "                    dtrain,                        \n",
        "                    num_boost_round=20000,         # the number of boosting trees\n",
        "                    early_stopping_rounds=500,    # val loss가 계속 상승하면 중지\n",
        "                    nfold=5,                      # set folds of the closs validation\n",
        "                    verbose_eval=300,             # 몇 번째마다 메세지를 출력할 것인지\n",
        "    #                    feval=rmse_exp,               # price 속성을 log scaling 했기 때문에, 다시 exponential\n",
        "                    maximize=False,\n",
        "                    show_stdv=False,              # 학습 동안 std(표준편차) 출력할지 말지\n",
        "                    )\n",
        "\n",
        "# [2700]\ttrain-rmse:28041.2\ttest-rmse:116153\n",
        "# ch_model(cleaned_train)\n",
        "def clean_data(dataset):\n",
        "\n",
        "    # Explo data\n",
        "    # print('Raw Dataset shape :'.ljust(36), 'col', dataset.shape[0], 'row', dataset.shape[1])\n",
        "    null_list = {}\n",
        "    for i in train.columns:\n",
        "        colnull = train[i].isnull().sum()\n",
        "        if not colnull == 0:\n",
        "            null_list[i] = colnull\n",
        "    # print('missing value colnames and counts : '.ljust(36), null_list)\n",
        "\n",
        "    # date column\n",
        "    dataset['data_y'] = ''\n",
        "    dataset['data_m'] = ''\n",
        "    dataset['data_y'] = dataset['date'].apply(lambda x : str(x[:4])).astype(int)\n",
        "    dataset['data_m'] = dataset['date'].apply(lambda x : str(x[4:6])).astype(int)\n",
        "    dataset.drop('date', axis=1, inplace=True)\n",
        "    # print('date dropped Dataset shape :'.ljust(36), 'col', dataset.shape[0], 'row', dataset.shape[1])\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def geogege(data):\n",
        "    data['zipcode'] = data['zipcode'].astype(str)  \n",
        "    data['zipcode-3'] = data['zipcode'].apply(lambda x : str(x[2:3])).astype(int)\n",
        "    data['zipcode-4'] = data['zipcode'].apply(lambda x : str(x[3:4])).astype(int)\n",
        "    data['zipcode-5'] = data['zipcode'].apply(lambda x : str(x[4:5])).astype(int)\n",
        "    data['zipcode-34'] = data['zipcode'].apply(lambda x : str(x[2:4])).astype(int)\n",
        "    data['zipcode-45'] = data['zipcode'].apply(lambda x : str(x[3:5])).astype(int)\n",
        "    data['zipcode-35'] = data['zipcode'].apply(lambda x : str(x[2:5])).astype(int)\n",
        "    data.drop('zipcode', axis=1, inplace=True)\n",
        "    return data\n",
        "\n",
        "\n",
        "def ppcca2(trainset, testset):\n",
        "    pca2 = PCA(n_components=2)\n",
        "    coord = trainset[['lat','long']]\n",
        "    coord_test = testset[['lat','long']]\n",
        "    \n",
        "    principalComponents_updated = pca2.fit_transform(coord)\n",
        "    trainset['coord_pca1']= ''\n",
        "    trainset['coord_pca2']= ''\n",
        "    trainset['coord_pca1']= principalComponents_updated[:, 0]\n",
        "    trainset['coord_pca2']= principalComponents_updated[:, 1]\n",
        "\n",
        "    principalComponents_updated_test = pca2.transform(coord_test)\n",
        "    testset['coord_pca1']= ''\n",
        "    testset['coord_pca2']= ''\n",
        "    testset['coord_pca1']= principalComponents_updated_test[:, 0]\n",
        "    testset['coord_pca2']= principalComponents_updated_test[:, 1]\n",
        "    return trainset, testset\n",
        "\n",
        "\n",
        "def ppcca1(trainset, testset):\n",
        "    pca1 = PCA(n_components=2)\n",
        "\n",
        "    principalComponents_updated = pca1.fit_transform(trainset)\n",
        "    trainset['pca1']= ''\n",
        "    trainset['pca2']= ''\n",
        "    trainset['pca1']= principalComponents_updated[:, 0]\n",
        "    trainset['pca2']= principalComponents_updated[:, 1]\n",
        "\n",
        "    principalComponents_updated_test = pca1.transform(testset)\n",
        "    testset['pca1']= ''\n",
        "    testset['pca2']= ''\n",
        "    testset['pca1']= principalComponents_updated_test[:, 0]\n",
        "    testset['pca2']= principalComponents_updated_test[:, 1]\n",
        "    return trainset, testset\n",
        "\n",
        "\n",
        "def Skewed_CF(dataset):\n",
        "    skewed = ['sqft_living', 'sqft_lot', 'sqft_living15', 'sqft_lot15', 'sqft_above', 'sqft_basement']\n",
        "    features_log_transformed = pd.DataFrame(data = dataset)\n",
        "    features_log_transformed[skewed] = dataset[skewed].apply(lambda x: np.log(x + 1))\n",
        "    return features_log_transformed\n",
        "\n",
        "\n",
        "def scaler_dummy(dataset,dataset_test):\n",
        "\n",
        "    scaler_mm = MinMaxScaler() \n",
        "    scaler_ma = MaxAbsScaler()\n",
        "    scaler_sd = StandardScaler()\n",
        "    scaler_rb = RobustScaler()\n",
        "\n",
        "    numerical = list(dataset.columns)\n",
        "    data_transform_mm = pd.DataFrame(data = dataset)\n",
        "    data_transform_ma = pd.DataFrame(data = dataset)\n",
        "    data_transform_sd = pd.DataFrame(data = dataset)\n",
        "    data_transform_rb = pd.DataFrame(data = dataset)\n",
        "\n",
        "    data_transform_mm[numerical] = scaler_mm.fit_transform(dataset[numerical])\n",
        "    data_transform_ma[numerical] = scaler_ma.fit_transform(dataset[numerical])\n",
        "    data_transform_sd[numerical] = scaler_sd.fit_transform(dataset[numerical])\n",
        "    data_transform_rb[numerical] = scaler_rb.fit_transform(dataset[numerical])\n",
        "  #     scaler_mm.fit(dataset[numerical])\n",
        "  #     scaler_ma.fit(dataset[numerical])\n",
        "  #     scaler_sd.fit(dataset[numerical])\n",
        "  #     scaler_rb.fit(dataset[numerical])\n",
        "\n",
        "    data_transform_mm[numerical] = scaler_mm.transform(dataset[numerical])\n",
        "    data_transform_ma[numerical] = scaler_ma.transform(dataset[numerical])\n",
        "    data_transform_sd[numerical] = scaler_sd.transform(dataset[numerical])\n",
        "    data_transform_rb[numerical] = scaler_rb.transform(dataset[numerical])\n",
        "\n",
        "    ## get dummies\n",
        "    features_final_mm = pd.get_dummies(data_transform_mm)\n",
        "    features_final_ma = pd.get_dummies(data_transform_ma)\n",
        "    features_final_sd = pd.get_dummies(data_transform_sd)\n",
        "    features_final_rb = pd.get_dummies(data_transform_rb)\n",
        "\n",
        "    numerical = list(dataset_test.columns)\n",
        "    scaler_mm_fitted_test = scaler_mm.transform(dataset_test[numerical])\n",
        "    scaler_ma_fitted_test = scaler_ma.transform(dataset_test[numerical])\n",
        "    scaler_sd_fitted_test = scaler_sd.transform(dataset_test[numerical])\n",
        "    scaler_rb_fitted_test = scaler_rb.transform(dataset_test[numerical])\n",
        "\n",
        "    scaler_mm_fitted_test = pd.DataFrame(data = scaler_mm_fitted_test,columns=numerical)\n",
        "    scaler_ma_fitted_test = pd.DataFrame(data = scaler_ma_fitted_test,columns=numerical)\n",
        "    scaler_sd_fitted_test = pd.DataFrame(data = scaler_sd_fitted_test,columns=numerical)\n",
        "    scaler_rb_fitted_test = pd.DataFrame(data = scaler_rb_fitted_test,columns=numerical)\n",
        "    \n",
        "    features_final_mmt = pd.get_dummies(scaler_mm_fitted_test)\n",
        "    features_final_mat = pd.get_dummies(scaler_ma_fitted_test)\n",
        "    features_final_sdt = pd.get_dummies(scaler_sd_fitted_test)\n",
        "    features_final_rbt = pd.get_dummies(scaler_rb_fitted_test)        \n",
        "    return features_final_mm, features_final_ma, features_final_sd, features_final_rb, features_final_mmt, features_final_mat, features_final_sdt, features_final_rbt\n",
        "  \n",
        "\n",
        "def get_oof(clf, x_train, y_train, x_test, eval_func, **kwargs):\n",
        "    nfolds = kwargs.get('NFOLDS', 5)\n",
        "    kfold_shuffle = kwargs.get('kfold_shuffle', True)\n",
        "    kfold_random_state = kwargs.get('kfold_random_state', 0)\n",
        "    stratified_kfold_ytrain = kwargs.get('stratifed_kfold_y_value', None)\n",
        "    ntrain = x_train.shape[0]\n",
        "    ntest = x_test.shape[0]\n",
        "    \n",
        "    kf_split = None\n",
        "    if stratified_kfold_ytrain is None:\n",
        "        kf = KFold(n_splits=nfolds, shuffle=kfold_shuffle, random_state=kfold_random_state)\n",
        "        kf_split = kf.split(x_train)\n",
        "    else:\n",
        "        kf = StratifiedKFold(n_splits=nfolds, shuffle=kfold_shuffle, random_state=kfold_random_state)\n",
        "        kf_split = kf.split(x_train, stratified_kfold_ytrain)\n",
        "        \n",
        "    oof_train = np.zeros((ntrain,))\n",
        "    oof_test = np.zeros((ntest,))\n",
        "\n",
        "    cv_sum = 0\n",
        "    \n",
        "    # before running model, print model param\n",
        "    # lightgbm model and xgboost model use get_params()\n",
        "    try:\n",
        "        if clf.clf is not None:\n",
        "            print(clf.clf)\n",
        "    except:\n",
        "        print(clf)\n",
        "        print(clf.get_params())\n",
        "\n",
        "    for i, (train_index, cross_index) in enumerate(kf_split):\n",
        "        x_tr, x_cr = None, None\n",
        "        y_tr, y_cr = None, None\n",
        "        if isinstance(x_train, pd.DataFrame):\n",
        "            x_tr, x_cr = x_train.iloc[train_index], x_train.iloc[cross_index]\n",
        "            y_tr, y_cr = y_train.iloc[train_index], y_train.iloc[cross_index]\n",
        "        else:\n",
        "            x_tr, x_cr = x_train[train_index], x_train[cross_index]\n",
        "            y_tr, y_cr = y_train[train_index], y_train[cross_index]\n",
        "\n",
        "        clf.train(x_tr, y_tr, x_cr, y_cr)\n",
        "        \n",
        "        oof_train[cross_index] = clf.predict(x_cr)\n",
        "\n",
        "        cv_score = eval_func(y_cr, oof_train[cross_index])\n",
        "        \n",
        "        print('Fold %d / ' % (i+1), 'CV-Score: %.6f' % cv_score)\n",
        "        cv_sum = cv_sum + cv_score\n",
        "        \n",
        "        del x_tr, x_cr, y_tr, y_cr\n",
        "        \n",
        "    gc.collect()\n",
        "    \n",
        "    score = cv_sum / nfolds\n",
        "    print(\"Average CV-Score: \", score)\n",
        "\n",
        "    # Using All Dataset, retrain\n",
        "    clf.train(x_train, y_train)\n",
        "    oof_test = clf.predict(x_test)\n",
        "\n",
        "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1), score\n",
        "\n",
        "\n",
        "def time_decorator(func): \n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        print(\"\\nStartTime: \", datetime.now() + timedelta(hours=9))\n",
        "        start_time = time.time()\n",
        "        \n",
        "        df = func(*args, **kwargs)\n",
        "        \n",
        "        print(\"EndTime: \", datetime.now() + timedelta(hours=9))  \n",
        "        print(\"TotalTime: \", time.time() - start_time)\n",
        "        return df\n",
        "        \n",
        "    return wrapper\n",
        "\n",
        "\n",
        "class SklearnWrapper(object):\n",
        "    def __init__(self, clf, params=None, **kwargs):\n",
        "        #if isinstance(SVR) is False:\n",
        "        #    params['random_state'] = kwargs.get('seed', 0)\n",
        "        self.clf = clf(**params)\n",
        "        self.is_classification_problem = True\n",
        "    @time_decorator\n",
        "    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n",
        "        if len(np.unique(y_train)) > 30:\n",
        "            self.is_classification_problem = False\n",
        "            \n",
        "        self.clf.fit(x_train, y_train)\n",
        "\n",
        "    def predict(self, x):\n",
        "        if self.is_classification_problem is True:\n",
        "            return self.clf.predict_proba(x)[:,1]\n",
        "        else:\n",
        "            return self.clf.predict(x)   \n",
        "\n",
        "\n",
        "class XgbWrapper(object):\n",
        "    def __init__(self, params=None, **kwargs):\n",
        "        self.param = params\n",
        "        self.param['seed'] = kwargs.get('seed', RANDOM_SEED)\n",
        "        self.num_rounds = kwargs.get('num_rounds', 1000)\n",
        "        self.early_stopping = kwargs.get('ealry_stopping', 100)\n",
        "\n",
        "        self.eval_function = kwargs.get('eval_function', None)\n",
        "        self.verbose_eval = kwargs.get('verbose_eval', 100)\n",
        "        self.best_round = 0\n",
        "    \n",
        "    @time_decorator\n",
        "    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n",
        "        need_cross_validation = True\n",
        "       \n",
        "        if isinstance(y_train, pd.DataFrame) is True:\n",
        "            y_train = y_train[y_train.columns[0]]\n",
        "            if y_cross is not None:\n",
        "                y_cross = y_cross[y_cross.columns[0]]\n",
        "\n",
        "        if x_cross is None:\n",
        "            dtrain = xgb.DMatrix(x_train, label=y_train, silent= True)\n",
        "            train_round = self.best_round\n",
        "            if self.best_round == 0:\n",
        "                train_round = self.num_rounds\n",
        "            \n",
        "            print(train_round)\n",
        "            self.clf = xgb.train(self.param, dtrain, train_round)\n",
        "            del dtrain\n",
        "        else:\n",
        "            dtrain = xgb.DMatrix(x_train, label=y_train, silent=True)\n",
        "            dvalid = xgb.DMatrix(x_cross, label=y_cross, silent=True)\n",
        "            watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
        "\n",
        "            self.clf = xgb.train(self.param, dtrain, self.num_rounds, watchlist, feval=self.eval_function,\n",
        "                                 early_stopping_rounds=self.early_stopping,\n",
        "                                 verbose_eval=self.verbose_eval)\n",
        "            self.best_round = max(self.best_round, self.clf.best_iteration)\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.clf.predict(xgb.DMatrix(x), ntree_limit=self.best_round)\n",
        "\n",
        "    def get_params(self):\n",
        "        return self.param    \n",
        "\n",
        "\n",
        "class LgbmWrapper(object):\n",
        "    def __init__(self, params=None, **kwargs):\n",
        "        self.param = params\n",
        "        self.param['seed'] = kwargs.get('seed', RANDOM_SEED)\n",
        "        self.num_rounds = kwargs.get('num_rounds', 1000)\n",
        "        self.early_stopping = kwargs.get('ealry_stopping', 100)\n",
        "\n",
        "        self.eval_function = kwargs.get('eval_function', None)\n",
        "        self.verbose_eval = kwargs.get('verbose_eval', 100)\n",
        "        self.best_round = 0\n",
        "        \n",
        "    @time_decorator\n",
        "    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n",
        "        \"\"\"\n",
        "        x_cross or y_cross is None\n",
        "        -> model train limted num_rounds\n",
        "        \n",
        "        x_cross and y_cross is Not None\n",
        "        -> model train using validation set\n",
        "        \"\"\"\n",
        "        if isinstance(y_train, pd.DataFrame) is True:\n",
        "            y_train = y_train[y_train.columns[0]]\n",
        "            if y_cross is not None:\n",
        "                y_cross = y_cross[y_cross.columns[0]]\n",
        "\n",
        "        if x_cross is None:\n",
        "            dtrain = lgb.Dataset(x_train, label=y_train, silent= True)\n",
        "            train_round = self.best_round\n",
        "            if self.best_round == 0:\n",
        "                train_round = self.num_rounds\n",
        "                \n",
        "            self.clf = lgb.train(self.param, train_set=dtrain, num_boost_round=train_round)\n",
        "            del dtrain   \n",
        "        else:\n",
        "            dtrain = lgb.Dataset(x_train, label=y_train, silent=True)\n",
        "            dvalid = lgb.Dataset(x_cross, label=y_cross, silent=True)\n",
        "            self.clf = lgb.train(self.param, train_set=dtrain, num_boost_round=self.num_rounds, valid_sets=[dtrain, dvalid],\n",
        "                                  feval=self.eval_function, early_stopping_rounds=self.early_stopping,\n",
        "                                  verbose_eval=self.verbose_eval)\n",
        "            self.best_round = max(self.best_round, self.clf.best_iteration)\n",
        "            del dtrain, dvalid\n",
        "            \n",
        "        gc.collect()\n",
        "    \n",
        "    def predict(self, x):\n",
        "        return self.clf.predict(x, num_iteration=self.clf.best_iteration)\n",
        "    \n",
        "    def plot_importance(self):\n",
        "        lgb.plot_importance(self.clf, max_num_features=50, height=0.7, figsize=(10,30))\n",
        "        plt.show()\n",
        "        \n",
        "    def get_params(self):\n",
        "        return self.param\n",
        "\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(np.expm1(y_true), np.expm1(y_pred)))\n",
        "\n",
        "\n",
        "gbr_param = {'alpha':0.9, 'criterion':'friedman_mse', 'init':None, 'learning_rate':0.1, 'loss':'ls', 'max_depth':5,\n",
        "        'max_features':None, 'max_leaf_nodes':None,\n",
        "        'min_impurity_decrease':0.0, 'min_impurity_split':None,\n",
        "        'min_samples_leaf':1, 'min_samples_split':2,\n",
        "        'min_weight_fraction_leaf':0.0, 'n_estimators':200,\n",
        "        'n_iter_no_change':None, 'presort':'auto', 'random_state':RANDOM_SEED,\n",
        "        'subsample':1.0, 'tol':0.1, 'validation_fraction':0.1, 'verbose':0\n",
        "        }\n",
        "\n",
        "xgb_params = {\n",
        "    'eval_metric': 'rmse',\n",
        "    'seed': RANDOM_SEED,\n",
        "    'eta': 0.05,\n",
        "    'gamma':0,\n",
        "    'max_depth':5,\n",
        "    'reg_alpha':0.00006,\n",
        "    'subsample': 0.9,\n",
        "    'colsample_bytree': 0.4,\n",
        "    'silent': 1,\n",
        "}\n",
        "\n",
        "xgb_params1 = {\n",
        "    'eval_metric': 'rmse',\n",
        "    'seed': RANDOM_SEED,\n",
        "    'eta': 0.0123,\n",
        "    'gamma':0,\n",
        "    'max_depth':3,\n",
        "    'reg_alpha':0.00006,\n",
        "    'subsample': 0.7,\n",
        "    'colsample_bytree': 0.7,\n",
        "    'silent': 1,\n",
        "        # 'seed': RANDOM_SEED,\n",
        "        # 'learning_rate': 0.05,\n",
        "        # 'max_depth': 5,\n",
        "        # 'subsample': 0.9,\n",
        "        # 'colsample_bytree': 0.4,\n",
        "        # 'silent': True,\n",
        "        # # 'gpu_id':0 ,         \n",
        "        # # 'tree_method':'gpu_hist',\n",
        "        # # 'predictor':'gpu_predictor',\n",
        "        # 'n_estimators':5000,\n",
        "        # 'refit' : True\n",
        "}\n",
        "\n",
        "lgb_params7 = {'num_leaves': 10,\n",
        "         'min_data_in_leaf': 10, \n",
        "         'objective':'regression',\n",
        "         'max_depth': -1,\n",
        "         'learning_rate': 0.05,\n",
        "         \"min_child_samples\": 10,\n",
        "         \"boosting\": \"gbdt\",\n",
        "         \"feature_fraction\": 0.9,\n",
        "         \"bagging_freq\": 1,\n",
        "         \"bagging_fraction\": 0.9 ,\n",
        "         \"bagging_seed\": 11,\n",
        "         \"metric\": 'rmse',\n",
        "         \"lambda_l1\": 0.1,\n",
        "         \"verbosity\": -1,\n",
        "         \"nthread\": 4,\n",
        "         'n_estimators':5000,\n",
        "             'max_bin' : 100,\n",
        "        #  'refit':True, \n",
        "    'tree_method':'gpu_hist',\n",
        "    'predictor':'gpu_predictor',\n",
        "         \"random_state\": RANDOM_SEED}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q3OX4_uxHMdN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# type_checkk(train)\n",
        "cleaned_train = clean_data(train)\n",
        "cleaned_test = clean_data(test)\n",
        "\n",
        "geoge_train = geogege(cleaned_train)\n",
        "geoge_test = geogege(cleaned_test)\n",
        "\n",
        "pcaed_train, pcaed_test = ppcca2(geoge_train, geoge_test)\n",
        "pcaed_train1, pcaed_test1 = ppcca1(pcaed_train, pcaed_test)\n",
        "\n",
        "# Skewed_train = Skewed_CF(pcaed_train1)\n",
        "# Skewed_test = Skewed_CF(pcaed_test1)\n",
        "# mm, ma, sd, rb, mmt, mat, sdt, rbt = scaler_dummy(Skewed_train, Skewed_test)\n",
        "\n",
        "\n",
        "# mm, ma, sd, rb\n",
        "# x_train, x_test, y_train, y_test = train_test_split(sd, price_raw_log, test_size = 0.1, random_state=RANDOM_SEED)\n",
        "\n",
        "x_train = pcaed_train1\n",
        "y_train = price_raw_log\n",
        "x_test = pcaed_test1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MD9XIebaHqjd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gbr_model = SklearnWrapper(GradientBoostingRegressor, params=gbr_param)\n",
        "xgb_model = XgbWrapper(params=xgb_params, num_rounds = 20000, ealry_stopping=500, verbose_eval=300)\n",
        "lgb_model = LgbmWrapper(params=lgb_params7, num_rounds = 20000, ealry_stopping=500, verbose_eval=300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3ff1Py79IClG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_columns = [col for col in x_train.columns if col not in ['id','price']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iZ_nWUx_Hxz-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3925
        },
        "outputId": "d972196f-1d9c-4254-f994-c6e26554cafd"
      },
      "cell_type": "code",
      "source": [
        "xgb_train, xgb_test, xgb_cv_score = get_oof(xgb_model, x_train[train_columns], y_train, x_test[train_columns], \n",
        "                            rmse, NFOLDS=5, kfold_random_state=RANDOM_SEED)\n",
        "\n",
        "lgb_train, lgb_test, lgb_cv_score = get_oof(lgb_model, x_train[train_columns], y_train, x_test[train_columns], \n",
        "                            rmse, NFOLDS=5, kfold_random_state=RANDOM_SEED)\n",
        "\n",
        "gbr_train, gbr_test, lasso_cv_score = get_oof(gbr_model, x_train[train_columns].fillna(-1), y_train, x_test[train_columns].fillna(-1), \n",
        "                            rmse, NFOLDS=5, kfold_random_state=RANDOM_SEED)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.XgbWrapper object at 0x7f714d26bba8>\n",
            "{'eval_metric': 'rmse', 'seed': 631, 'eta': 0.05, 'gamma': 0, 'max_depth': 5, 'reg_alpha': 6e-05, 'subsample': 0.9, 'colsample_bytree': 0.4, 'silent': 1}\n",
            "\n",
            "StartTime:  2019-04-22 23:25:33.739946\n",
            "[0]\ttrain-rmse:11.9323\teval-rmse:11.9326\n",
            "Multiple eval metrics have been passed: 'eval-rmse' will be used for early stopping.\n",
            "\n",
            "Will train until eval-rmse hasn't improved in 500 rounds.\n",
            "[300]\ttrain-rmse:0.133125\teval-rmse:0.16005\n",
            "[600]\ttrain-rmse:0.112541\teval-rmse:0.155976\n",
            "[900]\ttrain-rmse:0.098501\teval-rmse:0.155033\n",
            "[1200]\ttrain-rmse:0.087329\teval-rmse:0.154922\n",
            "[1500]\ttrain-rmse:0.078014\teval-rmse:0.155039\n",
            "Stopping. Best iteration:\n",
            "[1026]\ttrain-rmse:0.093558\teval-rmse:0.154724\n",
            "\n",
            "EndTime:  2019-04-22 23:26:35.102941\n",
            "TotalTime:  61.36314916610718\n",
            "Fold 1 /  CV-Score: 100929.101475\n",
            "\n",
            "StartTime:  2019-04-22 23:26:35.218232\n",
            "[0]\ttrain-rmse:11.9351\teval-rmse:11.9208\n",
            "Multiple eval metrics have been passed: 'eval-rmse' will be used for early stopping.\n",
            "\n",
            "Will train until eval-rmse hasn't improved in 500 rounds.\n",
            "[300]\ttrain-rmse:0.133536\teval-rmse:0.157972\n",
            "[600]\ttrain-rmse:0.112736\teval-rmse:0.154904\n",
            "[900]\ttrain-rmse:0.098694\teval-rmse:0.154333\n",
            "[1200]\ttrain-rmse:0.087694\teval-rmse:0.154363\n",
            "Stopping. Best iteration:\n",
            "[917]\ttrain-rmse:0.098012\teval-rmse:0.154216\n",
            "\n",
            "EndTime:  2019-04-22 23:26:49.373928\n",
            "TotalTime:  14.155787229537964\n",
            "Fold 2 /  CV-Score: 127426.426447\n",
            "\n",
            "StartTime:  2019-04-22 23:26:49.481818\n",
            "[0]\ttrain-rmse:11.933\teval-rmse:11.9295\n",
            "Multiple eval metrics have been passed: 'eval-rmse' will be used for early stopping.\n",
            "\n",
            "Will train until eval-rmse hasn't improved in 500 rounds.\n",
            "[300]\ttrain-rmse:0.133545\teval-rmse:0.1578\n",
            "[600]\ttrain-rmse:0.112613\teval-rmse:0.154673\n",
            "[900]\ttrain-rmse:0.098491\teval-rmse:0.154291\n",
            "[1200]\ttrain-rmse:0.087275\teval-rmse:0.154106\n",
            "[1500]\ttrain-rmse:0.077932\teval-rmse:0.154425\n",
            "Stopping. Best iteration:\n",
            "[1209]\ttrain-rmse:0.086938\teval-rmse:0.154084\n",
            "\n",
            "EndTime:  2019-04-22 23:27:06.507335\n",
            "TotalTime:  17.025611877441406\n",
            "Fold 3 /  CV-Score: 100116.591439\n",
            "\n",
            "StartTime:  2019-04-22 23:27:06.649598\n",
            "[0]\ttrain-rmse:11.9275\teval-rmse:11.9522\n",
            "Multiple eval metrics have been passed: 'eval-rmse' will be used for early stopping.\n",
            "\n",
            "Will train until eval-rmse hasn't improved in 500 rounds.\n",
            "[300]\ttrain-rmse:0.133003\teval-rmse:0.161976\n",
            "[600]\ttrain-rmse:0.112758\teval-rmse:0.158396\n",
            "[900]\ttrain-rmse:0.09838\teval-rmse:0.157491\n",
            "[1200]\ttrain-rmse:0.087437\teval-rmse:0.157142\n",
            "[1500]\ttrain-rmse:0.078359\teval-rmse:0.156798\n",
            "[1800]\ttrain-rmse:0.070231\teval-rmse:0.156764\n",
            "[2100]\ttrain-rmse:0.063357\teval-rmse:0.156971\n",
            "Stopping. Best iteration:\n",
            "[1886]\ttrain-rmse:0.068102\teval-rmse:0.156714\n",
            "\n",
            "EndTime:  2019-04-22 23:27:31.936370\n",
            "TotalTime:  25.287028551101685\n",
            "Fold 4 /  CV-Score: 119244.517456\n",
            "\n",
            "StartTime:  2019-04-22 23:27:32.194289\n",
            "[0]\ttrain-rmse:11.9334\teval-rmse:11.9261\n",
            "Multiple eval metrics have been passed: 'eval-rmse' will be used for early stopping.\n",
            "\n",
            "Will train until eval-rmse hasn't improved in 500 rounds.\n",
            "[300]\ttrain-rmse:0.132141\teval-rmse:0.16589\n",
            "[600]\ttrain-rmse:0.111579\teval-rmse:0.162463\n",
            "[900]\ttrain-rmse:0.097663\teval-rmse:0.161889\n",
            "[1200]\ttrain-rmse:0.086731\teval-rmse:0.16166\n",
            "[1500]\ttrain-rmse:0.077405\teval-rmse:0.161867\n",
            "Stopping. Best iteration:\n",
            "[1042]\ttrain-rmse:0.092256\teval-rmse:0.161566\n",
            "\n",
            "EndTime:  2019-04-22 23:27:47.496548\n",
            "TotalTime:  15.30225682258606\n",
            "Fold 5 /  CV-Score: 109285.795913\n",
            "Average CV-Score:  111400.48654607737\n",
            "\n",
            "StartTime:  2019-04-22 23:27:47.775339\n",
            "1886\n",
            "EndTime:  2019-04-22 23:28:08.780545\n",
            "TotalTime:  21.005910873413086\n",
            "<__main__.LgbmWrapper object at 0x7f714d26bcf8>\n",
            "{'num_leaves': 10, 'min_data_in_leaf': 10, 'objective': 'regression', 'max_depth': -1, 'learning_rate': 0.05, 'min_child_samples': 10, 'boosting': 'gbdt', 'feature_fraction': 0.9, 'bagging_freq': 1, 'bagging_fraction': 0.9, 'bagging_seed': 11, 'metric': 'rmse', 'lambda_l1': 0.1, 'verbosity': -1, 'nthread': 4, 'n_estimators': 5000, 'max_bin': 100, 'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor', 'random_state': 631, 'seed': 631}\n",
            "\n",
            "StartTime:  2019-04-22 23:28:09.294195\n",
            "Training until validation scores don't improve for 500 rounds.\n",
            "[300]\ttraining's rmse: 0.150123\tvalid_1's rmse: 0.163512\n",
            "[600]\ttraining's rmse: 0.135667\tvalid_1's rmse: 0.160204\n",
            "[900]\ttraining's rmse: 0.125854\tvalid_1's rmse: 0.158153\n",
            "[1200]\ttraining's rmse: 0.11809\tvalid_1's rmse: 0.157654\n",
            "[1500]\ttraining's rmse: 0.111381\tvalid_1's rmse: 0.157626\n",
            "Early stopping, best iteration is:\n",
            "[1284]\ttraining's rmse: 0.116123\tvalid_1's rmse: 0.157441\n",
            "EndTime:  2019-04-22 23:28:14.245568\n",
            "TotalTime:  4.951456069946289\n",
            "Fold 1 /  CV-Score: 112568.843619\n",
            "\n",
            "StartTime:  2019-04-22 23:28:14.381610\n",
            "Training until validation scores don't improve for 500 rounds.\n",
            "[300]\ttraining's rmse: 0.15091\tvalid_1's rmse: 0.160979\n",
            "[600]\ttraining's rmse: 0.136402\tvalid_1's rmse: 0.156631\n",
            "[900]\ttraining's rmse: 0.126348\tvalid_1's rmse: 0.155067\n",
            "[1200]\ttraining's rmse: 0.118156\tvalid_1's rmse: 0.154756\n",
            "[1500]\ttraining's rmse: 0.111448\tvalid_1's rmse: 0.154408\n",
            "[1800]\ttraining's rmse: 0.105377\tvalid_1's rmse: 0.15417\n",
            "[2100]\ttraining's rmse: 0.100102\tvalid_1's rmse: 0.154239\n",
            "[2400]\ttraining's rmse: 0.0951497\tvalid_1's rmse: 0.154244\n",
            "Early stopping, best iteration is:\n",
            "[1968]\ttraining's rmse: 0.102368\tvalid_1's rmse: 0.154038\n",
            "EndTime:  2019-04-22 23:28:21.100651\n",
            "TotalTime:  6.719572305679321\n",
            "Fold 2 /  CV-Score: 126831.837771\n",
            "\n",
            "StartTime:  2019-04-22 23:28:21.277423\n",
            "Training until validation scores don't improve for 500 rounds.\n",
            "[300]\ttraining's rmse: 0.150907\tvalid_1's rmse: 0.160188\n",
            "[600]\ttraining's rmse: 0.136132\tvalid_1's rmse: 0.15668\n",
            "[900]\ttraining's rmse: 0.126149\tvalid_1's rmse: 0.155498\n",
            "[1200]\ttraining's rmse: 0.117986\tvalid_1's rmse: 0.15495\n",
            "[1500]\ttraining's rmse: 0.111249\tvalid_1's rmse: 0.154472\n",
            "[1800]\ttraining's rmse: 0.105245\tvalid_1's rmse: 0.154528\n",
            "Early stopping, best iteration is:\n",
            "[1546]\ttraining's rmse: 0.110158\tvalid_1's rmse: 0.154408\n",
            "EndTime:  2019-04-22 23:28:26.924647\n",
            "TotalTime:  5.647288799285889\n",
            "Fold 3 /  CV-Score: 103208.366444\n",
            "\n",
            "StartTime:  2019-04-22 23:28:27.072745\n",
            "Training until validation scores don't improve for 500 rounds.\n",
            "[300]\ttraining's rmse: 0.149752\tvalid_1's rmse: 0.166286\n",
            "[600]\ttraining's rmse: 0.1357\tvalid_1's rmse: 0.161891\n",
            "[900]\ttraining's rmse: 0.125703\tvalid_1's rmse: 0.159749\n",
            "[1200]\ttraining's rmse: 0.117863\tvalid_1's rmse: 0.15876\n",
            "[1500]\ttraining's rmse: 0.111199\tvalid_1's rmse: 0.158527\n",
            "[1800]\ttraining's rmse: 0.105365\tvalid_1's rmse: 0.158469\n",
            "[2100]\ttraining's rmse: 0.100098\tvalid_1's rmse: 0.158286\n",
            "[2400]\ttraining's rmse: 0.0952863\tvalid_1's rmse: 0.15796\n",
            "[2700]\ttraining's rmse: 0.0908847\tvalid_1's rmse: 0.157959\n",
            "[3000]\ttraining's rmse: 0.0867411\tvalid_1's rmse: 0.158149\n",
            "Early stopping, best iteration is:\n",
            "[2663]\ttraining's rmse: 0.0914125\tvalid_1's rmse: 0.157906\n",
            "EndTime:  2019-04-22 23:28:37.430678\n",
            "TotalTime:  10.358222007751465\n",
            "Fold 4 /  CV-Score: 123372.978908\n",
            "\n",
            "StartTime:  2019-04-22 23:28:37.729011\n",
            "Training until validation scores don't improve for 500 rounds.\n",
            "[300]\ttraining's rmse: 0.148833\tvalid_1's rmse: 0.169511\n",
            "[600]\ttraining's rmse: 0.13462\tvalid_1's rmse: 0.16541\n",
            "[900]\ttraining's rmse: 0.124806\tvalid_1's rmse: 0.163952\n",
            "[1200]\ttraining's rmse: 0.116939\tvalid_1's rmse: 0.163271\n",
            "[1500]\ttraining's rmse: 0.110161\tvalid_1's rmse: 0.162836\n",
            "[1800]\ttraining's rmse: 0.104249\tvalid_1's rmse: 0.162666\n",
            "[2100]\ttraining's rmse: 0.0989691\tvalid_1's rmse: 0.162805\n",
            "[2400]\ttraining's rmse: 0.094156\tvalid_1's rmse: 0.162849\n",
            "Early stopping, best iteration is:\n",
            "[1939]\ttraining's rmse: 0.10173\tvalid_1's rmse: 0.162525\n",
            "EndTime:  2019-04-22 23:28:44.824681\n",
            "TotalTime:  7.095900535583496\n",
            "Fold 5 /  CV-Score: 109037.796823\n",
            "Average CV-Score:  115003.96471292987\n",
            "\n",
            "StartTime:  2019-04-22 23:28:45.098488\n",
            "EndTime:  2019-04-22 23:28:59.017998\n",
            "TotalTime:  13.920092582702637\n",
            "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
            "             learning_rate=0.1, loss='ls', max_depth=5, max_features=None,\n",
            "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
            "             min_impurity_split=None, min_samples_leaf=1,\n",
            "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "             n_estimators=200, n_iter_no_change=None, presort='auto',\n",
            "             random_state=631, subsample=1.0, tol=0.1,\n",
            "             validation_fraction=0.1, verbose=0, warm_start=False)\n",
            "\n",
            "StartTime:  2019-04-22 23:29:00.028000\n",
            "EndTime:  2019-04-22 23:29:05.791109\n",
            "TotalTime:  5.763601303100586\n",
            "Fold 1 /  CV-Score: 106788.584351\n",
            "\n",
            "StartTime:  2019-04-22 23:29:05.812128\n",
            "EndTime:  2019-04-22 23:29:11.548254\n",
            "TotalTime:  5.736526727676392\n",
            "Fold 2 /  CV-Score: 126207.185227\n",
            "\n",
            "StartTime:  2019-04-22 23:29:11.569944\n",
            "EndTime:  2019-04-22 23:29:17.239411\n",
            "TotalTime:  5.669553995132446\n",
            "Fold 3 /  CV-Score: 109015.699150\n",
            "\n",
            "StartTime:  2019-04-22 23:29:17.260857\n",
            "EndTime:  2019-04-22 23:29:23.035139\n",
            "TotalTime:  5.774170398712158\n",
            "Fold 4 /  CV-Score: 123079.459539\n",
            "\n",
            "StartTime:  2019-04-22 23:29:23.056490\n",
            "EndTime:  2019-04-22 23:29:28.839953\n",
            "TotalTime:  5.783398866653442\n",
            "Fold 5 /  CV-Score: 118724.025807\n",
            "Average CV-Score:  116762.99081471628\n",
            "\n",
            "StartTime:  2019-04-22 23:29:28.946863\n",
            "EndTime:  2019-04-22 23:29:36.158637\n",
            "TotalTime:  7.212208032608032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jcPCiocbH9g-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train_second_layer = np.concatenate((lgb_train, xgb_train, gbr_train), axis=1)\n",
        "x_test_second_layer = np.concatenate((lgb_test, xgb_test, gbr_test), axis=1)\n",
        "\n",
        "x_train_second_layer = pd.DataFrame(x_train_second_layer)\n",
        "x_test_second_layer = pd.DataFrame(x_test_second_layer)\n",
        "\n",
        "x_train_second_layer.to_csv('train_oof.csv', index=False)\n",
        "x_test_second_layer.to_csv('test_oof.csv', index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OfusPXzQYcU9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#en\n",
        "\n",
        "* xgb  - Average CV-Score:  112067.39010682635\n",
        "* lgm - Average CV-Score:  118583.62096791738\n",
        "* gbr - Average CV-Score:  119439.6253779529\n",
        "\n",
        "\n",
        "\n",
        "* en_ans22 = xgb_ans*0.6 + lgb_ans*0.2 + gbr_ans*0.2\n",
        "* en_ans11 = xgb_ans*0.8 + lgb_ans*0.1 + gbr_ans*0.1"
      ]
    },
    {
      "metadata": {
        "id": "UD1Rf_dMdNes",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Average CV-Score:  111400.48654607737\n",
        "* Average CV-Score:  115003.96471292987\n",
        "* Average CV-Score:  116762.99081471628\n",
        "\n",
        "### 로컬\n",
        "* Average CV-Score:  111412.25640145992\n",
        "* Average CV-Score:  115003.96471292987\n",
        "* Average CV-Score:  116886.48316366393"
      ]
    },
    {
      "metadata": {
        "id": "_01TsPHIdhOZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "2751f817-ab38-43a4-c33f-644d4cad64b9"
      },
      "cell_type": "code",
      "source": [
        "q1 = 111400.48654607737\n",
        "q2 = 115003.96471292987\n",
        "q3 = 116762.99081471628\n",
        "\n",
        "a1 = (q1+q2+q3)/q1\n",
        "print(a1)\n",
        "a2 = (q1+q2+q3)/q2\n",
        "a3 = (q1+q2+q3)/q3\n",
        "print(a2)\n",
        "print(a3)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.080484230486578\n",
            "2.983961839318582\n",
            "2.939008667723097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SbIzv7-9eAwZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "xgb_ans = np.expm1(xgb_test)\n",
        "lgb_ans = np.expm1(lgb_test)\n",
        "gbr_ans = np.expm1(gbr_test)\n",
        "\n",
        "en_ans432 = xgb_ans*0.4 + lgb_ans*0.3 + gbr_ans*0.2\n",
        "en_ans622 = xgb_ans*0.6 + lgb_ans*0.2 + gbr_ans*0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HRXuB4DeYeK3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv(\"sample_submission.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4O04pVHrZ3Bg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6507d90d-d7da-4db1-acfd-a8bd2387218b"
      },
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv(\"sample_submission.csv\")\n",
        "len(submission)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6468"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "TPMstMg5Z4ak",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "submission['price']= en_ans432\n",
        "submission.to_csv('en_ans432.csv',index=False)\n",
        "\n",
        "submission['price']= en_ans622\n",
        "submission.to_csv('en_ans622.csv',index=False)\n",
        "\n",
        "submission['price']= xgb_ans\n",
        "submission.to_csv('xgb_ans.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a0kZhHCtZiAh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "en_ans911 = xgb_ans*0.9 + lgb_ans*0.05 + gbr_ans*0.05\n",
        "submission['price']= en_ans911\n",
        "submission.to_csv('en_ans911.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9e9KhfE1g_6m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "en_ans973 = xgb_ans*0.9 + lgb_ans*0.07 + gbr_ans*0.03\n",
        "submission['price']= en_ans973\n",
        "submission.to_csv('en_ans973.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gYLARRy1g_82",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "en_ans991 = xgb_ans*0.9 + lgb_ans*0.09 + gbr_ans*0.01\n",
        "submission['price']= en_ans991\n",
        "submission.to_csv('en_ans991.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GGQIrp8spmx5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}