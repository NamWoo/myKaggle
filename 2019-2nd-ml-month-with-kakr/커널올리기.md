**대단한 점수와 실력이 아니지만**   
**`단일 모델`로 `정규화`, `로그화` 없이 유의미한 결과를 내본 저의 경험을 공유합니다.**
* 스케일링, 노멀라이징 모두 **뺐**습니다. (평가결과가 훨씬 더 높았습니다)
* XGB 단일 모델 사용했습니다.

# 진행방법
1. 최초 xgb 모델 기준으로 데이터 특징이 변경되거나 추가변수(파생변수) 나올 때마다 평가 후 적용했습니다.
2. 괜찮으면 적용시키고 안좋으면 윗단 아랫단 작업순서를 바꿔가면서 평가했습니다.
3. 어느 정도 점수가 올라왔을 때 모델 파라미터를 GridSearch와 수작업(?)으로 찾아다녔습니다.
4. 모델은 XGB, LGBM, GBR 세 가지를 비교 분석했습니다.
    * XGBoost
    * LGBM
    * GradientBoostingRegression

# 결론정리
## 특성
### Date
* 특성은 `object` 타입이라 바꿔줘야 했던 `date`를 `년도`와 `달`별로 쪼개서 추가했습니다.
* 평가결과 `118848` 까지 왔습니다.

### 지역정보 zipcode
* 상식적으로 집값에 핵심은 지역이란 생각으로 접근했습니다.
* `zipcode` 별로 묶어서 카테고리 0 ~ 10까지 특성을 추가했었는데 [비슷하고 또 더 깔끔하게 정리하신 커널 참고](https://www.kaggle.com/tmheo74/geo-data-eda-and-feature-engineering#PCA-Transformation---Lat,-Long)했습니다.
* 평가결과 `116464` 까지 왔습니다.

## 변형
당연하듯 진행했던 과정들을 과감히 뺐습니다.

### Scaler
* 특이하게도 스케일링 하니까 점수가 많이 떨어졌습니다.
* `MinMaxScaler`, `MaxAbsScaler`, `StandardScaler`, `RobustScaler` 모두 비교분석했습니다.
* `MaxAbsScaler` 가 점수가 미비하게 높았지만 나중에 후반으로 가면서 비슷해져 모두 제외했습니다.

### log
* 분포가 정규분포그래프로 멋지게 그려지지 않고 쏠려있으면 대부분 log를 씌워서 그래프를 이쁘게 만드는데
* 평가결과가 훨씬 좋지 못했습니다.
* 수치형 특성들에 대해서 또 크게 쏠려있는 특성들에 대해서 이리저리 시도했지만 마찬가지로 별로였습니다.

## PCA
### 지역정보 lat, long
* 위에서와 마찬가지로 집값의 핵심은 지역이란 생각에 PCA로 클러스터링 접근했습니다. 
* 더 깔끔하게 정리하신 [허진명님 커널 참고하여 추가했습니다.](https://www.kaggle.com/tmheo74/geo-data-eda-and-feature-engineering#PCA-Transformation---Lat,-Long)
* 평가결과 2000 정도 상승이 있었습니다.

### 전항목 PCA 
* 전 특성을 PCA 클러스터링을 통해 차원축소를 했습니다.
* 2차원부터 18차원까지 비교했는데 2차원부터는 상승폭이 크지 않아 2차원으로 축소해서 추가했습니다.
* 평가결과 2000 정도 상승이 있었습니다

## 모델
* 평가결과 `XGB` >> `LGBM` > `GBR`
* Average CV-Score
  * 111400.48654607737 - `XGB`
  * 115003.96471292987 - `LGBM`
  * 116762.99081471628 - `GBR`
* XGB가 월등히 높진 않았지만 그래도 역시나 높았습니다.
  
## seed
* 개인적으로 다른 곳에서 억지로 끌어올리던 점수를 `seed` 하나바꿈으로 결과가 확확 바뀌었습니다.
* 점수 4000, 5000 이 오르락 내리락..






### DATE
* `date` 특성이 유일하게 `object` 타입이라 변환해야했습니다.
* `연` 따로 `달` 따로 추가했습니다.
* 기존 `date`는 없앴습니다. 
* 점수가 2000 점 정도 오



## 변환
* Normalizing Numerical Features - 정규화 안했습니다. 결과가 많이 안좋아졌습니다.
* Transforming Skewed Continuous Features - log화 안했습니다. 마찬가지로 점수 하강이 심했습니다.

## 모델
* 뭘 어떻게 하든 XGB 점수가 확실히 높았습니다.
* seed 값에 따라 결과 차이도 엄청났습니다.
  * 최초에 GradientBoostingRegression 모델로 결과값이 압도적이었는데 후반에 가서야 간신히 앞지를 수 있었습니다. 
  * 많은 파라메터를 바꿔보고 시도해봤지만 GradientBoostingRegression로만 높았던 것은 seed 우연이었던 것 같습니다.
  * 아무리 시도해도 다시 XGB 보다 높아지지 않았기에..




## Feature Engineering 

* 딱 1개 date 특성만 `object` 타입이라 변환했습니다
  
    dataset['data_y'] = ''
    dataset['data_m'] = ''
    dataset['data_y'] = dataset['date'].apply(lambda x : str(x[:4])).astype(int)
    dataset['data_m'] = dataset['date'].apply(lambda x : str(x[4:6])).astype(int)
    dataset.drop('date', axis=1, inplace=True)




# 진행요약

1. 최초 xgb 기준으로 데이터 특징이 변경되거나 추가변수(파생변수) 나올때 마다 평가 후 결과 확인했습니다.
2. 괜찮으면 적용시키고 나쁘면 pass
3. 안좋으면 윗단 아랫단 작업순서를 바꿔가면서 결과를 확인했습니다.
4. 어느 정도 올라왔을 때 파라미터를 GridSearch와 수작업(?)으로 찾아다녔습니다.
    * Colab은 20분? 30분 사용하지 않으면 초기화되면서 나가지기 때문에 조심해야했습니다.
5. 모델은 XGB, LGBM, GBR 세 가지를 비교 분석했습니다.
    * XGBoost
    * LGBM
    * GradientBoostingRegression

# 결과 


# 의문
* 최초에 GradientBoostingRegression 하나로 XGB 모델 점수까지 올라왔었는데 아무리 다시 근